# -*- coding: utf-8 -*-
"""Model V2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtQsukp0ctu7uMwOT6YbDLEE0mlfszvk
"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
import os
from tqdm import tqdm
import time

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import ConvLSTM2D,Conv2DTranspose, LayerNormalization, BatchNormalization, TimeDistributed, Conv2D, Flatten, Dense, Dropout
#from keras_layer_normalization import LayerNormalization
import keras
import pprint

physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

class Config():
  def __init__(self, train_path, test_path, model_path, img_size = (128, 128), batch_size = 8, mx_frm = 1600, stride = [1, 2], frm_cnt = 10, test_size = 400, epochs = 10):
    self.train_path = train_path
    self.test_path = test_path
    self.img_size = img_size
    self.batch_size = batch_size
    self.model_path = model_path
    self.epochs = epochs
    self.stride = stride
    self.mx_frm = mx_frm
    self.frm_cnt = frm_cnt
    self.test_size = test_size

class Functions(Config):
  def __init__(self):
    Config.__init__(self, train_path, test_path, model_path)
    self.load_buffer = {'frm_cnt': None, 'indx':0, 'total':0}

  def load_batch(self):
    clips = []
    a = 0
    q = 0
    for dir in tqdm(os.walk(train_path)):
      a += 1
      if a == 1:
        continue

      try:

        if not self.load_buffer['frm_cnt']:
          self.load_buffer['indx'] += 1
          self.load_buffer['total'] += 1
        pth = os.path.join(dir[0], sorted(dir[2])[self.load_buffer['indx']])
        clips.append(self.load_frames(pth))

      except Exception as e:
        print(e)
        self.load_buffer['indx'] = 0
        continue

      break
    return clips, self.load_buffer['total']

  def load_frames(self, pth, agmt = True):
    video = cv2.VideoCapture(pth)
    print('\n starting video no : ',self.load_buffer['total'])
    frames = []
    cnt = 0

    while video.isOpened:
      ret, frame = video.read()
      cnt += 1
      if not ret:
        print('\nTotal frames read', cnt)
        self.load_buffer['frm_cnt'] = None
        print("\nvideo finished.")
        break

      if self.load_buffer['frm_cnt']:
        if self.load_buffer['frm_cnt'] <= cnt:
          img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
          frame = cv2.resize(img/256, self.img_size)
        else:
          continue

      else:
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame = cv2.resize(img/256, self.img_size)
      #print('frame shape = ', frame.shape)
      frames.append(frame.reshape([self.img_size[0], self.img_size[1], 1]))

      if len(frames) >= self.mx_frm:
        break

    if ret:
      self.load_buffer['frm_cnt'] = cnt
    video.release()

    if len(frames) < self.frm_cnt:
      print('video has insufficiant frames')
      self.load_buffer['frm_cnt'] = None
      raise
    if agmt:
      frames = self.augment(frames)
    return np.array(frames)

  def augment(self, frames):
    agmted = np.zeros((self.frm_cnt, self.img_size[0], self.img_size[1], 1))
    clips = []

    try:
      for strd in self.stride:
        for s in range(0, len(frames), strd):
          if len(frames[s:s+self.frm_cnt]) == 10:
            agmted[:,:,:,:] = frames[s:s+self.frm_cnt]
            clips. append(agmted)
    except:
      print('Error occured in augment')

    no = len(clips) % 8
    print("clips dropped ",no)
    clips = clips[:len(clips)-no]
    return clips

  def load_single_test(self):
    test = np.zeros((self.test_size, self.img_size[0], self.img_size[1], 1))
    for dir in os.listdir(self.test_path):
      path = os.path.join(self.test_path, dir)
      frames = self.load_frames(path, agmt = False)

    test = frames[0:self.test_size]
    del frames
    return test
"""Model"""

class Model(Functions):
  def __init__(self):
    Functions.__init__(self)
    self.output1 = None
    self.output = None

  def anom(self):
    inputs = tf.keras.layers.Input(shape=[self.frm_cnt, self.img_size[0], self.img_size[1], 1])
    encode = [
              self.spatial(64, (5,5), stride = 2, pading="same", cnv=True),
              self.temporal(64, (3,3), pading='same'),
              self.temporal(32, (3,3), pading='same')
    ]
    decode = [
              self.temporal(64, (3,3), pading='same'),
              self.spatial(64,(5,5), stride = 2, pading="same", cnv = False),
              self.spatial(128, (11,11), stride= 2, pading="same", cnv= False)
    ]
    seq = tf.keras.Sequential()
    x = TimeDistributed(Conv2D(128, (11, 11), strides=4, padding="same"), batch_input_shape=(None, self.frm_cnt, self.img_size[0], self.img_size[1], 1))(inputs)
    x = LayerNormalization()(x)
    for enc in encode:
      x = enc(x)
    self.output1 = x

    for dec in decode:
      x = dec(x)

    output = TimeDistributed(Conv2D(1, (11, 11), activation="sigmoid", padding="same"))(x)

    return tf.keras.Model(inputs=inputs, outputs = output)

  def spatial(self, filters, filter_size,stride , cnv = True, pading="same"):
    seq = tf.keras.Sequential()
    if cnv:
      seq.add(TimeDistributed(Conv2D(filters, filter_size, padding=pading)))
    else:
      seq.add(TimeDistributed(Conv2DTranspose(filters, filter_size, strides=stride, padding=pading)))
    seq.add(LayerNormalization())
    return seq

  def temporal(self, filters, filter_size, pading = "same", return_sequence=True):
    seq = tf.keras.Sequential()
    seq.add(ConvLSTM2D(filters, filter_size, padding=pading, return_sequences=return_sequence))
    seq.add(LayerNormalization())
    return seq

  def anom_type(self):
    seq = Sequential()
    seq.add(Flatten())
    seq.add(Dense(1000, activation='relu'))
    seq.add(Dropout(0.5))
    seq.add(Dense(512, activation='relu'))
    seq.add(Dropout(0.4))
    seq.add(Dense(128, activation='relu'))
    seq.add(Dropout(0.5))
    seq.add(Dense(13, activation='softmax'))
    return seq

def evaluate(test):
    #model = load_model(model_path,custom_objects={'LayerNormalization': LayerNormalization})
    sz = test.shape[0] // 10
    sequences = np.zeros((sz, 10, img_dim[0], img_dim[1], 1))
    # apply the sliding window technique to get the sequences
    cnt = 0
    for i in range(0, test.shape[0], 10):
      if i + 10 <= test.shape[0]:
        sequences[cnt, :, :, :, :] = test[i:i+10]
        cnt += 1
    test = None
    clip = None
    # get the reconstruction cost of all the sequences
    reconstructed_sequences = model.predict(sequences,batch_size=4)
    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sequences[i],reconstructed_sequences[i])) for i in range(0,sz)])
    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)
    sr = 1.0 - sa
    #print(sr)
    try:
      fle = open('IRIS-ringtone/textFile/text.txt', 'w')
    except:
      time.sleep(3)
      fle = open('IRIS-ringtone/textFile/text.txt', 'w')
    if (sr<=0.96).any() or (sr<=0.96).all():
      fle.write('1')
      # print(time.clock())
    else:
      fle.write('0')
    fle.close()
    #plot the regularity scores
    # print(sr)
    # plt.plot(sr)
    # plt.ylabel('regularity score Sr(t)')
    # plt.xlabel('frame t')
    # plt.show()

def test(test_path):
  for pth in os.listdir(test_path):
    tst_pth = os.path.join(test_path, pth)
    frames = []
    vid = cv2.VideoCapture(tst_pth)
    n = 0
    while vid.isOpened():
      ret, frame = vid.read()
      if not ret:
        break
      n+= 1
      cv2.imshow('vid', frame)
      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
      frame = cv2.resize(frame/256, img_dim)
      frames.append(frame.reshape((img_dim[0],img_dim[1], 1)))
      if n == 300:
        # t0 = time.clock()
        frames = np.array(frames).reshape((300, img_dim[0], img_dim[1], 1))
        evaluate(frames)
        n = 0
        frames =[]
        # t1 = -t0 + time.clock()
        # print('Time taken to predict for 1 sec frames = ', t1)
      else:
        if cv2.waitKey(25) & 0xFF == ord('q'):
          break
    vid.release()
    cv2.destroyAllWindows()

if __name__ == '__main__':
  model_path = 'Model/tpu_model.h5'
  train_path = '/content/drive/My Drive/Anomaly detection/UCF/Anomaly-Detection-Dataset/Train'
  test_path = 'Test'
  cnfg = Config(train_path, test_path, model_path)
  fncn = Functions()
  mdl = Model()
  img_dim = (128, 128)
  '''while True:
    clips, ttl = fncn.load_batch()
    if ttl == 800:
      clips = None
      break
    #seq1.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=1e-4, decay=1e-5, epsilon=1e-6))
    seq1.fit(clips, clips,
              batch_size=cnfg.batch_size, epochs=cnfg.epochs, shuffle=False)
    seq1.save_weights('./tpu_model.h5', overwrite=True)

    if ttl % 5 == 0:
      seq = mdl.anom()
      seq.load_weights('./tpu_model.h5')
      seq.save(cnfg.model_path)
      seq = None
  '''
  model = mdl.anom()
  model.compile(loss='mse',experimental_steps_per_execution = 50, optimizer=tf.keras.optimizers.Adam(lr=1e-4, decay=1e-5, epsilon=1e-6))
  try:
    model.load_weights('Model/tpu_model.h5')
    print('Model loaded successfuly')
  except:
    print("couldn't load the weights")
  # model = load_mdl()
  test(test_path)
  # test= fncn.load_single_test()
  # evaluate(test)
